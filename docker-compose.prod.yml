
services:
  # PostgreSQL Database
  db:
    image: postgres:15-alpine
    container_name: oroshine_db
    restart: unless-stopped
    environment:
      POSTGRES_DB: ${PG_DB}
      POSTGRES_USER: ${PG_USER}
      POSTGRES_PASSWORD: ${PG_PASSWORD}
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - backend
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${PG_USER}"]
      interval: 30s
      timeout: 10s
      retries: 3
    # Resource limits for t2.micro
    mem_limit: 256m
    mem_reservation: 128m
    cpus: 0.3
    command: >
      postgres
      -c shared_buffers=128MB
      -c effective_cache_size=384MB
      -c maintenance_work_mem=64MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=4MB
      -c default_statistics_target=100
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
      -c work_mem=2621kB
      -c min_wal_size=1GB
      -c max_wal_size=4GB
      -c max_connections=50

  # Redis Cache
  redis:
    image: redis:7-alpine
    container_name: oroshine_redis
    restart: unless-stopped
    command: >
      redis-server
      --requirepass ${REDIS_PASSWORD}
      --maxmemory 128mb
      --maxmemory-policy allkeys-lru
      --save 60 1000
      --appendonly yes
      --tcp-backlog 128
    volumes:
      - redis_data:/data
    networks:
      - backend
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    mem_limit: 150m
    mem_reservation: 100m
    cpus: 0.2

  # Django Web Application
  web:
    build:
      context: .
      dockerfile: Dockerfile.prod
      args:
        PYTHON_VERSION: 3.11
    container_name: oroshine_web
    restart: unless-stopped
    command: >
      sh -c "python manage.py collectstatic --noinput &&
             python manage.py migrate --noinput &&
             gunicorn oroshine_app.wsgi:application
             --bind 0.0.0.0:8000
             --workers 2
             --threads 2
             --worker-class gthread
             --worker-tmp-dir /dev/shm
             --timeout 60
             --graceful-timeout 30
             --max-requests 1000
             --max-requests-jitter 100
             --log-level info
             --access-logfile -
             --error-logfile -"
    volumes:
      - ./:/app
      - static_volume:/app/staticfiles
      - media_volume:/app/media
    environment:
      - DEBUG=${DEBUG}
      - SECRET_KEY=${SECRET_KEY}
      - ALLOWED_HOSTS=${ALLOWED_HOSTS}
      - DATABASE_URL=postgresql://${PG_USER}:${PG_PASSWORD}@db:5432/${PG_DB}
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - backend
      - frontend
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health/"]
      interval: 30s
      timeout: 10s
      retries: 3
    mem_limit: 256m
    mem_reservation: 200m
    cpus: 0.4

  # Celery Worker
  celery_worker:
    build:
      context: .
      dockerfile: Dockerfile.prod
    container_name: oroshine_celery
    restart: unless-stopped
    command: >
      celery -A oroshine_app worker
      --loglevel=info
      --concurrency=1
      --max-tasks-per-child=50
      --time-limit=300
      --soft-time-limit=240
      --pool=solo
    volumes:
      - ./:/app
      - media_volume:/app/media
    environment:
      - DEBUG=${DEBUG}
      - SECRET_KEY=${SECRET_KEY}
    depends_on:
      - redis
      - db
    networks:
      - backend
    mem_limit: 200m
    mem_reservation: 150m
    cpus: 0.3

  # Celery Beat (Scheduler)
  celery_beat:
    build:
      context: .
      dockerfile: Dockerfile.prod
    container_name: oroshine_beat
    restart: unless-stopped
    command: >
      celery -A oroshine_app beat
      --loglevel=info
      --scheduler django_celery_beat.schedulers:DatabaseScheduler
    volumes:
      - ./:/app
    environment:
      - DEBUG=${DEBUG}
      - SECRET_KEY=${SECRET_KEY}
    depends_on:
      - redis
      - db
    networks:
      - backend
    mem_limit: 100m
    mem_reservation: 64m
    cpus: 0.1

  # Nginx Reverse Proxy
  nginx:
    image: nginx:alpine
    container_name: oroshine_nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
      - static_volume:/app/staticfiles:ro
      - media_volume:/app/media:ro
      - ./certbot/conf:/etc/letsencrypt:ro
      - ./certbot/www:/var/www/certbot:ro
    depends_on:
      - web
    networks:
      - frontend
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/health/"]
      interval: 30s
      timeout: 10s
      retries: 3
    mem_limit: 64m
    cpus: 0.1

networks:
  backend:
    driver: bridge
  frontend:
    driver: bridge

volumes:
  postgres_data:
  redis_data:
  static_volume:
  media_volume: